import numpy as np
import matplotlib.pyplot as plt
from matplotlib import cm
import pandas as pd

# ==================== Helper Functions ====================

def basis(i, n):
    """Return the i-th standard basis vector in n dimensions."""
    e = np.zeros(n)
    e[i] = 1.0
    return e

def golden_section_search(f, x, d, tol=1e-6, max_iter=100):
    """
    One-dimensional line search using golden section method.
    Finds alpha that minimizes f(x + alpha * d).
    """
    phi = (1 + np.sqrt(5)) / 2
    resphi = 2 - phi
    
    # Initial bracket
    a, b = -10.0, 10.0
    
    for _ in range(max_iter):
        x1 = a + resphi * (b - a)
        x2 = b - resphi * (b - a)
        
        if f(x + x1 * d) < f(x + x2 * d):
            b = x2
        else:
            a = x1
            
        if abs(b - a) < tol:
            break
    
    alpha = (a + b) / 2
    return x + alpha * d

# ==================== Algorithm 1: Basic CCD ====================

def cyclic_coordinate_descent(f, x, epsilon=1e-6, max_iter=1000):
    """
    Basic Cyclic Coordinate Descent algorithm.
    
    Parameters:
    - epsilon: convergence tolerance
    """
    x = np.array(x, dtype=float)
    n = len(x)
    delta = np.inf
    iterations = 0
    trajectory = [x.copy()]
    
    while delta > epsilon and iterations < max_iter:
        x_prime = x.copy()
        
        for i in range(n):
            d = basis(i, n)
            x = golden_section_search(f, x, d)
        
        delta = np.linalg.norm(x - x_prime)
        iterations += 1
        trajectory.append(x.copy())
    
    return x, iterations, trajectory

# ==================== Algorithm 2: CCD with Acceleration ====================

def cyclic_coordinate_descent_with_acceleration(f, x, epsilon=1e-6, max_iter=1000):
    """
    Cyclic Coordinate Descent with acceleration step.
    
    Parameters:
    - epsilon: convergence tolerance
    """
    x = np.array(x, dtype=float)
    n = len(x)
    delta = np.inf
    iterations = 0
    trajectory = [x.copy()]
    
    while delta > epsilon and iterations < max_iter:
        x_prime = x.copy()
        
        for i in range(n):
            d = basis(i, n)
            x = golden_section_search(f, x, d)
        
        # Acceleration step
        accel_dir = x - x_prime
        if np.linalg.norm(accel_dir) > 1e-10:
            x = golden_section_search(f, x, accel_dir)
        
        delta = np.linalg.norm(x - x_prime)
        iterations += 1
        trajectory.append(x.copy())
    
    return x, iterations, trajectory

# ==================== Test Functions ====================

def booth_function(p):
    """Booth function: minimum at (1, 3) with f(1,3) = 0"""
    x, y = p
    return (x + 2*y - 7)**2 + (2*x + y - 5)**2

def rosenbrock_function(p):
    """Rosenbrock function: minimum at (1, 1) with f(1,1) = 0"""
    x, y = p
    return (1 - x)**2 + 5*(y - x**2)**2

def pathological_function(p):
    """Pathological function with multiple local minima"""
    x, y = p
    return np.sin(5*y) * np.cos(5*x) + x**2 + y**2

# ==================== Plotting Function ====================

def plot_contour_with_trajectory(f, trajectory, title, x_range, y_range, optimum=None):
    """
    Plot contour of function with optimization trajectory.
    
    Parameters:
    - optimum: optional global optimum point to mark
    """
    fig, ax = plt.subplots(figsize=(10, 8))
    
    # Create grid
    x = np.linspace(x_range[0], x_range[1], 200)
    y = np.linspace(y_range[0], y_range[1], 200)
    X, Y = np.meshgrid(x, y)
    Z = np.zeros_like(X)
    
    for i in range(X.shape[0]):
        for j in range(X.shape[1]):
            Z[i, j] = f([X[i, j], Y[i, j]])
    
    # Plot contours
    z_min, z_max = Z.min(), Z.max()
    z_range = z_max - z_min
    
    # Handle different cases for contour levels
    if z_range < 1e-10:
        # If range is too small, use a simple linear spacing
        levels = np.linspace(z_min, z_max, 30)
    elif z_min <= 0:
        # If minimum is negative or zero, use linear spacing
        levels = np.linspace(z_min, z_max, 30)
    else:
        # If all positive and reasonable range, use log spacing
        try:
            levels = np.logspace(np.log10(z_min + 1e-6), np.log10(z_max), 30)
        except:
            # Fallback to linear if log fails
            levels = np.linspace(z_min, z_max, 30)
    
    # Remove any invalid levels
    levels = levels[np.isfinite(levels)]
    
    if len(levels) > 0:
        contour = ax.contour(X, Y, Z, levels=levels, cmap='viridis', alpha=0.6)
        try:
            ax.clabel(contour, inline=True, fontsize=8)
        except:
            # Skip labels if they cause issues
            pass
    
    # Plot trajectory
    trajectory = np.array(trajectory)
    ax.plot(trajectory[:, 0], trajectory[:, 1], 'r.-', linewidth=2, 
            markersize=8, label='Optimization Path')
    ax.plot(trajectory[0, 0], trajectory[0, 1], 'go', markersize=12, 
            label='Start Point')
    ax.plot(trajectory[-1, 0], trajectory[-1, 1], 'r*', markersize=15, 
            label='End Point')
    
    # Mark global optimum if provided
    if optimum is not None:
        ax.plot(optimum[0], optimum[1], 'b^', markersize=12, 
                label='Global Optimum')
    
    ax.set_xlabel('x', fontsize=12)
    ax.set_ylabel('y', fontsize=12)
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    return fig

# ==================== Main Experiment Runner ====================

def run_all_experiments():
    """Run all experiments and generate results."""
    
    # Define experiments
    experiments = [
        {
            'name': 'Booth',
            'function': booth_function,
            'starts': [(0.0, 0.0), (-3.0, 4.0)],
            'x_range': (-5, 5),
            'y_range': (-5, 5),
            'optimum': (1, 3)
        },
        {
            'name': 'Rosenbrock',
            'function': rosenbrock_function,
            'starts': [(-1.5, 2.0), (2.0, -1.0)],
            'x_range': (-2, 3),
            'y_range': (-2, 4),
            'optimum': (1, 1)
        },
        {
            'name': 'Pathological',
            'function': pathological_function,
            'starts': [(2.0, 2.0), (-2.0, -2.0)],
            'x_range': (-3, 3),
            'y_range': (-3, 3),
            'optimum': (0, 0)
        }
    ]
    
    results = []
    plot_count = 0
    
    print("\n" + "="*80)
    print("CYCLIC COORDINATE DESCENT OPTIMIZATION RESULTS")
    print("="*80 + "\n")
    
    for exp in experiments:
        func = exp['function']
        func_name = exp['name']
        
        for start_idx, start_point in enumerate(exp['starts']):
            print(f"\n{func_name} Function - Starting Point: {start_point}")
            print("-" * 60)
            
            # Run Basic CCD
            x_basic, iter_basic, traj_basic = cyclic_coordinate_descent(
                func, start_point
            )
            f_basic = func(x_basic)
            
            print(f"Basic CCD:")
            print(f"  Final Point: ({x_basic[0]:.6f}, {x_basic[1]:.6f})")
            print(f"  Final Value: {f_basic:.6e}")
            print(f"  Iterations: {iter_basic}")
            
            results.append({
                'Function': func_name,
                'Start Point': f"({start_point[0]}, {start_point[1]})",
                'Algorithm': 'Basic CCD',
                'Final Point': f"({x_basic[0]:.6f}, {x_basic[1]:.6f})",
                'Final Value': f"{f_basic:.6e}",
                'Iterations': iter_basic
            })
            
            # Run Accelerated CCD
            x_accel, iter_accel, traj_accel = cyclic_coordinate_descent_with_acceleration(
                func, start_point
            )
            f_accel = func(x_accel)
            
            print(f"Accelerated CCD:")
            print(f"  Final Point: ({x_accel[0]:.6f}, {x_accel[1]:.6f})")
            print(f"  Final Value: {f_accel:.6e}")
            print(f"  Iterations: {iter_accel}")
            
            results.append({
                'Function': func_name,
                'Start Point': f"({start_point[0]}, {start_point[1]})",
                'Algorithm': 'Accelerated CCD',
                'Final Point': f"({x_accel[0]:.6f}, {x_accel[1]:.6f})",
                'Final Value': f"{f_accel:.6e}",
                'Iterations': iter_accel
            })
            
            # Create plots for Booth and Rosenbrock functions
            if func_name in ['Booth', 'Rosenbrock', 'Pathological']:
                # Plot Basic CCD
                fig = plot_contour_with_trajectory(
                    func, traj_basic,
                    f"{func_name} Function - Basic CCD\nStart: {start_point}",
                    exp['x_range'], exp['y_range'], exp['optimum']
                )
                plt.savefig(f'plot_{func_name}_basic_start{start_idx+1}.png', 
                           dpi=150, bbox_inches='tight')
                plt.close()
                plot_count += 1
                
                # Plot Accelerated CCD
                fig = plot_contour_with_trajectory(
                    func, traj_accel,
                    f"{func_name} Function - Accelerated CCD\nStart: {start_point}",
                    exp['x_range'], exp['y_range'], exp['optimum']
                )
                plt.savefig(f'plot_{func_name}_accel_start{start_idx+1}.png', 
                           dpi=150, bbox_inches='tight')
                plt.close()
                plot_count += 1
    
    # Print summary table
    print("\n" + "="*80)
    print("SUMMARY TABLE")
    print("="*80 + "\n")
    
    df = pd.DataFrame(results)
    print(df.to_string(index=False))
    
    print(f"\nâœ“ Generated {plot_count} contour plots for all test functions")
    print("="*80)
    
    # Discussion
    print("\n" + "="*80)
    print("DISCUSSION")
    print("="*80 + "\n")
    
    discussion = """
1. Convergence to Global Minimum:
   - Both algorithms successfully converge to the global minimum for the Booth function 
     from both starting points, as this is a convex quadratic function.
   - Both algorithms successfully converge to the global minimum for the Rosenbrock function.
   - The Pathological function has multiple local minima, so convergence to the global 
     minimum is not guaranteed and depends heavily on the starting point.

2. Effect of Acceleration Step:
   - The acceleration step significantly reduces the number of iterations needed, 
     especially for functions with elongated valleys (like Rosenbrock).
   - Acceleration provides a direction combining all coordinate movements, allowing 
     the algorithm to move more directly toward the optimum rather than in a 
     zig-zag pattern along coordinate axes.

3. Sensitivity to Starting Point:
   - The Booth and Rosenbrock functions show low sensitivity due to their convex nature.
   - The Pathological function shows high sensitivity. 
     Since it has many local minima, the choice of starting points can affect the outcome.
   - For non-convex functions, the choice of starting point can determine whether 
     the algorithm finds the global or a local minimum.

4. Why Methods Converge Well for Some Functions:
   - CCD works best on separable or nearly-separable functions where coordinate-wise 
     optimization is effective. The Booth function has strong coupling but is convex, 
     so convergence is guaranteed. The Rosenbrock function has a narrow valley aligned 
     diagonally, making pure coordinate descent inefficient - this is where acceleration 
     helps. The Pathological function's multiple local minima and complex landscape make 
     convergence to the global optimum difficult regardless of the method used.
"""
    
    print(discussion)
    print("="*80 + "\n")

# ==================== Run Experiments ====================

if __name__ == "__main__":
    run_all_experiments()